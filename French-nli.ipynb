{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "934ed815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306cd498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_DATASETS_CACHE=\"/data/users/sgarg6/hf_cache\"\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"# Modify this as per your environment\\n%env HF_DATASETS_CACHE=\\\"/data/users/sgarg6/hf_cache\\\"\";\n",
       "                var nbb_formatted_code = \"# Modify this as per your environment\\n%env HF_DATASETS_CACHE=\\\"/data/users/sgarg6/hf_cache\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify this as per your environment\n",
    "%env HF_DATASETS_CACHE=\"/data/users/sgarg6/hf_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa6a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"/data/users/sgarg6/hf_cache\"\r\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"!echo $HF_DATASETS_CACHE\";\n",
       "                var nbb_formatted_code = \"!echo $HF_DATASETS_CACHE\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!echo $HF_DATASETS_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dbaf82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"import torch\\nimport os\\n\\ndevice = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\";\n",
       "                var nbb_formatted_code = \"import torch\\nimport os\\n\\ndevice = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a690065",
   "metadata": {},
   "source": [
    "# Code to translate the snli dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddb805",
   "metadata": {},
   "source": [
    "Code has been commented as it's no longer required. The dataset has been pushed to HF_hub and is loaded from there for all the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae0c15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"# from datasets import load_dataset\\n\\n\\n# from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\n# dataset = load_dataset('snli')\\n# tokenizer = T5Tokenizer.from_pretrained(\\\"t5-small\\\")\\n# model = T5ForConditionalGeneration.from_pretrained(\\\"t5-small\\\").to(device)\\n\\n# def translate(sentence):\\n#     sentence = [\\\"translate English to French: \\\" + sent for sent in sentence]\\n#     input_ids = tokenizer(\\n#         sentence, return_tensors=\\\"pt\\\", truncation=True, padding=True\\n#     ).input_ids.to(device)\\n#     outputs = model.generate(input_ids)\\n#     return tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n\\n# def add_translations(example):\\n#     example[\\\"translated_premise\\\"] = translate(example[\\\"premise\\\"])\\n#     example[\\\"translated_hypothesis\\\"] = translate(example[\\\"hypothesis\\\"])\\n#     return example\\n\\n\\n# updated_dataset = dataset.map(add_translations, batched=True, batch_size=128)\\n\\n\\n# updated_dataset.save_to_disk(\\n#    os.path.join(os.environ[\\\"HF_DATASETS_CACHE\\\"], \\\"french-snli.hf\\\")\\n# )\\n\\n# dataset = datasets.load_from_disk(os.path.join(os.environ[\\\"HF_DATASETS_CACHE\\\"], \\\"french-snli.hf\\\"))\\n# updated_dataset.push_to_hub(\\\"sugam11/french-snli\\\")\";\n",
       "                var nbb_formatted_code = \"# from datasets import load_dataset\\n\\n\\n# from transformers import T5Tokenizer, T5ForConditionalGeneration\\n\\n# dataset = load_dataset('snli')\\n# tokenizer = T5Tokenizer.from_pretrained(\\\"t5-small\\\")\\n# model = T5ForConditionalGeneration.from_pretrained(\\\"t5-small\\\").to(device)\\n\\n# def translate(sentence):\\n#     sentence = [\\\"translate English to French: \\\" + sent for sent in sentence]\\n#     input_ids = tokenizer(\\n#         sentence, return_tensors=\\\"pt\\\", truncation=True, padding=True\\n#     ).input_ids.to(device)\\n#     outputs = model.generate(input_ids)\\n#     return tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n\\n# def add_translations(example):\\n#     example[\\\"translated_premise\\\"] = translate(example[\\\"premise\\\"])\\n#     example[\\\"translated_hypothesis\\\"] = translate(example[\\\"hypothesis\\\"])\\n#     return example\\n\\n\\n# updated_dataset = dataset.map(add_translations, batched=True, batch_size=128)\\n\\n\\n# updated_dataset.save_to_disk(\\n#    os.path.join(os.environ[\\\"HF_DATASETS_CACHE\\\"], \\\"french-snli.hf\\\")\\n# )\\n\\n# dataset = datasets.load_from_disk(os.path.join(os.environ[\\\"HF_DATASETS_CACHE\\\"], \\\"french-snli.hf\\\"))\\n# updated_dataset.push_to_hub(\\\"sugam11/french-snli\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# dataset = load_dataset('snli')\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "\n",
    "# def translate(sentence):\n",
    "#     sentence = [\"translate English to French: \" + sent for sent in sentence]\n",
    "#     input_ids = tokenizer(\n",
    "#         sentence, return_tensors=\"pt\", truncation=True, padding=True\n",
    "#     ).input_ids.to(device)\n",
    "#     outputs = model.generate(input_ids)\n",
    "#     return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "# def add_translations(example):\n",
    "#     example[\"translated_premise\"] = translate(example[\"premise\"])\n",
    "#     example[\"translated_hypothesis\"] = translate(example[\"hypothesis\"])\n",
    "#     return example\n",
    "\n",
    "\n",
    "# updated_dataset = dataset.map(add_translations, batched=True, batch_size=128)\n",
    "\n",
    "\n",
    "# updated_dataset.save_to_disk(\n",
    "#    os.path.join(os.environ[\"HF_DATASETS_CACHE\"], \"french-snli.hf\")\n",
    "# )\n",
    "\n",
    "# dataset = datasets.load_from_disk(os.path.join(os.environ[\"HF_DATASETS_CACHE\"], \"french-snli.hf\"))\n",
    "# updated_dataset.push_to_hub(\"sugam11/french-snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8d58ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ae3d441cff44308bceacf54279a641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/768 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration sugam11--french-snli-18806dba64b311d4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /soe/sgarg6/course_work/244_nlp/french-nli/\"/data/users/sgarg6/hf_cache\"/sugam11___parquet/sugam11--french-snli-18806dba64b311d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3582059213b401b80ea845abe41bdf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5bfb513b3747949f4cf2e671682e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/38.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2048736ca1d4439f8a55709111e870b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/795k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b39200fae0460f88d024d93ba69df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83851f5fb4b948348c9a865ea7c4f5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/550152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /soe/sgarg6/course_work/244_nlp/french-nli/\"/data/users/sgarg6/hf_cache\"/sugam11___parquet/sugam11--french-snli-18806dba64b311d4/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4633a830bd41898185592a674e085e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"from datasets import load_dataset\\n# dataset = datasets.load_from_disk(os.path.join(os.environ[\\\"HF_DATASETS_CACHE\\\"], \\\"french-snli.hf\\\"))\\ndataset = load_dataset(\\\"sugam11/french-snli\\\")\";\n",
       "                var nbb_formatted_code = \"from datasets import load_dataset\\n\\n# dataset = datasets.load_from_disk(os.path.join(os.environ[\\\"HF_DATASETS_CACHE\\\"], \\\"french-snli.hf\\\"))\\ndataset = load_dataset(\\\"sugam11/french-snli\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# dataset = datasets.load_from_disk(os.path.join(os.environ[\"HF_DATASETS_CACHE\"], \"french-snli.hf\"))\n",
    "dataset = load_dataset(\"sugam11/french-snli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fadd3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'translated_premise', 'translated_hypothesis'],\n",
       "        num_rows: 550152\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'translated_premise', 'translated_hypothesis'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'translated_premise', 'translated_hypothesis'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# Check if dataset contains all the relevant columns\\ndataset\";\n",
       "                var nbb_formatted_code = \"# Check if dataset contains all the relevant columns\\ndataset\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check if dataset contains all the relevant columns\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4463c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {1: \"NEUTRAL\", 0: \"ENTAILMENT\", 2: \"CONTRADICTION\"}\n",
    "label2id = {\"ENTAILMENT\": 0, \"NEUTRAL\": 1, \"CONTRADICTION\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1420dc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving 0 files to the new cache system\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96016936b3d4ddcbf56ce21579c54a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4434b32ae95c4d9889547368a025e3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/236 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b529c105392c498cad64cf843549a075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/732 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2ce1054cb64688810318634bdb644d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2097f2bc0814f3cadd10120e2d9396b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/273M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cmarkea/distilcamembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at cmarkea/distilcamembert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"cmarkea/distilcamembert-base\\\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    \\\"cmarkea/distilcamembert-base\\\", num_labels=3, id2label=id2label, label2id=label2id\\n)\";\n",
       "                var nbb_formatted_code = \"from transformers import AutoTokenizer, AutoModelForSequenceClassification\\n\\ntokenizer = AutoTokenizer.from_pretrained(\\\"cmarkea/distilcamembert-base\\\")\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    \\\"cmarkea/distilcamembert-base\\\", num_labels=3, id2label=id2label, label2id=label2id\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cmarkea/distilcamembert-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"cmarkea/distilcamembert-base\", num_labels=3, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b8ead6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"model = model.to(device)\";\n",
       "                var nbb_formatted_code = \"model = model.to(device)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5bc9c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943ba644e4e24ef6a1ef04af16eb0e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550152 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38ccc41fd5b4bbd8ab224bd05788e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81399eb84663419b89b688565780e635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# Modify dataset for NLI task\\n\\ndef combine_data(sample):\\n    # Combine hypothesis and premise and use CLS token for classification\\n    text = (\\n        tokenizer.cls_token\\n        + \\\" \\\"\\n        + sample[\\\"translated_premise\\\"]\\n        + \\\" \\\"\\n        + tokenizer.sep_token\\n        + \\\" \\\"\\n        + sample[\\\"translated_hypothesis\\\"]\\n    )\\n    sample[\\\"text\\\"] = text\\n    return sample\\n\\n\\ndataset = dataset.map(combine_data)\";\n",
       "                var nbb_formatted_code = \"# Modify dataset for NLI task\\n\\n\\ndef combine_data(sample):\\n    # Combine hypothesis and premise and use CLS token for classification\\n    text = (\\n        tokenizer.cls_token\\n        + \\\" \\\"\\n        + sample[\\\"translated_premise\\\"]\\n        + \\\" \\\"\\n        + tokenizer.sep_token\\n        + \\\" \\\"\\n        + sample[\\\"translated_hypothesis\\\"]\\n    )\\n    sample[\\\"text\\\"] = text\\n    return sample\\n\\n\\ndataset = dataset.map(combine_data)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modify dataset for NLI task\n",
    "\n",
    "\n",
    "def combine_data(sample):\n",
    "    # Combine hypothesis and premise and use CLS token for classification\n",
    "    text = (\n",
    "        tokenizer.cls_token\n",
    "        + \" \"\n",
    "        + sample[\"translated_premise\"]\n",
    "        + \" \"\n",
    "        + tokenizer.sep_token\n",
    "        + \" \"\n",
    "        + sample[\"translated_hypothesis\"]\n",
    "    )\n",
    "    sample[\"text\"] = text\n",
    "    return sample\n",
    "\n",
    "\n",
    "dataset = dataset.map(combine_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961b2d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313b455788b44f31984bb0994f5082af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4299 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d054840f15cc46c29c38fc1db88d29ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566b174ae08748eea150ab13fa36072b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"# Tokenize the dataset\\n\\ndataset = dataset.map(\\n    lambda x: tokenizer(x[\\\"text\\\"], truncation=True, padding=True),\\n    batched=True,\\n    batch_size=128,\\n)\";\n",
       "                var nbb_formatted_code = \"# Tokenize the dataset\\n\\ndataset = dataset.map(\\n    lambda x: tokenizer(x[\\\"text\\\"], truncation=True, padding=True),\\n    batched=True,\\n    batch_size=128,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the dataset\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda x: tokenizer(x[\"text\"], truncation=True, padding=True),\n",
    "    batched=True,\n",
    "    batch_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebf82035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419a07abe7ac4064a698fbc9c3a8c6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319ca90c90dc49b48533b03139188972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23821ba21ccb462b8fb726c56a3fa579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"# Discard data samples where label is -1\\n# This assumption is provided in the original SNLI dataset\\ndataset = dataset.filter(lambda sample: sample[\\\"label\\\"] >= 0)\";\n",
       "                var nbb_formatted_code = \"# Discard data samples where label is -1\\n# This assumption is provided in the original SNLI dataset\\ndataset = dataset.filter(lambda sample: sample[\\\"label\\\"] >= 0)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Discard data samples where label is -1\n",
    "# This assumption is provided in the original SNLI dataset\n",
    "dataset = dataset.filter(lambda sample: sample[\"label\"] >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c041e436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'translated_premise', 'translated_hypothesis', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 549367\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'translated_premise', 'translated_hypothesis', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9842\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'translated_premise', 'translated_hypothesis', 'text', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 9824\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"dataset\";\n",
       "                var nbb_formatted_code = \"dataset\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dfc802b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# convert dataset to tensors with only model inputs\\ndataset.set_format(type=\\\"pt\\\", columns=[\\\"input_ids\\\", \\\"attention_mask\\\", \\\"label\\\"])\";\n",
       "                var nbb_formatted_code = \"# convert dataset to tensors with only model inputs\\ndataset.set_format(type=\\\"pt\\\", columns=[\\\"input_ids\\\", \\\"attention_mask\\\", \\\"label\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# convert dataset to tensors with only model inputs\n",
    "dataset.set_format(type=\"pt\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e11a103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"dataset[\\\"train\\\"][\\\"label\\\"].unique()\";\n",
       "                var nbb_formatted_code = \"dataset[\\\"train\\\"][\\\"label\\\"].unique()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset[\"train\"][\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "59b3595f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hpnlhdqf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a437e60262304e7383c5e373dfd9d946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▅▆█▁▃▅▆█</td></tr><tr><td>train/global_step</td><td>▁▃▅▆█▁▃▅▆█</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▅▄▄▄▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.12</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.5726</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sunny-aardvark-4</strong> at: <a href='https://wandb.ai/sugam110795/nlp244/runs/hpnlhdqf' target=\"_blank\">https://wandb.ai/sugam110795/nlp244/runs/hpnlhdqf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230219_112435-hpnlhdqf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hpnlhdqf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c5401ff898440ba65be4e751ae9cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668950549016397, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/soe/sgarg6/course_work/244_nlp/french-nli/wandb/run-20230219_113853-5ww107vn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sugam110795/nlp244/runs/5ww107vn' target=\"_blank\">solar-wind-5</a></strong> to <a href='https://wandb.ai/sugam110795/nlp244' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sugam110795/nlp244' target=\"_blank\">https://wandb.ai/sugam110795/nlp244</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sugam110795/nlp244/runs/5ww107vn' target=\"_blank\">https://wandb.ai/sugam110795/nlp244/runs/5ww107vn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/sugam110795/nlp244/runs/5ww107vn?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f9c20e28430>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 38;\n",
       "                var nbb_unformatted_code = \"import wandb\\n\\nwandb.init(entity=\\\"sugam110795\\\", project=\\\"nlp244\\\", group=\\\"quest4\\\")\";\n",
       "                var nbb_formatted_code = \"import wandb\\n\\nwandb.init(entity=\\\"sugam110795\\\", project=\\\"nlp244\\\", group=\\\"quest4\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.init(entity=\"sugam110795\", project=\"nlp244\", group=\"quest4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1df7d9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"# Copied Code from https://github.com/kingb12/hf_libraries_demo\\n\\nfrom evaluate import Metric, EvaluationModuleInfo\\nfrom typing import Dict, Any\\nfrom sklearn.metrics import f1_score\\nimport evaluate\\n\\nclass MyMacroF1Metric(Metric):\\n    \\\"\\\"\\\"\\n    You can define custom metrics! In this case I do this to compute Macro-F1, which averages per-class F1 scores\\n    \\\"\\\"\\\"\\n\\n    f1_metric_info: EvaluationModuleInfo = evaluate.load(\\\"f1\\\")._info()\\n\\n    def _info(self) -> EvaluationModuleInfo:\\n        # we'll just say the info is the same in this case\\n        return MyMacroF1Metric.f1_metric_info\\n\\n    def _compute(\\n        self,\\n        predictions=None,\\n        references=None,\\n        labels=None,\\n        pos_label=1,\\n        sample_weight=None,\\n    ) -> Dict[str, Any]:\\n        # we can just call the sklearn implementation! Metrics in huggingface generally correspond with sklearn metrics\\n        # when applicable\\n        score = f1_score(\\n            references,\\n            predictions,\\n            labels=labels,\\n            pos_label=pos_label,\\n            average=\\\"macro\\\",\\n            sample_weight=sample_weight,\\n        )\\n        return {\\\"f1\\\": float(score) if score.size == 1 else score}\";\n",
       "                var nbb_formatted_code = \"# Copied Code from https://github.com/kingb12/hf_libraries_demo\\n\\nfrom evaluate import Metric, EvaluationModuleInfo\\nfrom typing import Dict, Any\\nfrom sklearn.metrics import f1_score\\nimport evaluate\\n\\n\\nclass MyMacroF1Metric(Metric):\\n    \\\"\\\"\\\"\\n    You can define custom metrics! In this case I do this to compute Macro-F1, which averages per-class F1 scores\\n    \\\"\\\"\\\"\\n\\n    f1_metric_info: EvaluationModuleInfo = evaluate.load(\\\"f1\\\")._info()\\n\\n    def _info(self) -> EvaluationModuleInfo:\\n        # we'll just say the info is the same in this case\\n        return MyMacroF1Metric.f1_metric_info\\n\\n    def _compute(\\n        self,\\n        predictions=None,\\n        references=None,\\n        labels=None,\\n        pos_label=1,\\n        sample_weight=None,\\n    ) -> Dict[str, Any]:\\n        # we can just call the sklearn implementation! Metrics in huggingface generally correspond with sklearn metrics\\n        # when applicable\\n        score = f1_score(\\n            references,\\n            predictions,\\n            labels=labels,\\n            pos_label=pos_label,\\n            average=\\\"macro\\\",\\n            sample_weight=sample_weight,\\n        )\\n        return {\\\"f1\\\": float(score) if score.size == 1 else score}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Copied Code from https://github.com/kingb12/hf_libraries_demo\n",
    "\n",
    "from evaluate import Metric, EvaluationModuleInfo\n",
    "from typing import Dict, Any\n",
    "from sklearn.metrics import f1_score\n",
    "import evaluate\n",
    "\n",
    "\n",
    "class MyMacroF1Metric(Metric):\n",
    "    \"\"\"\n",
    "    You can define custom metrics! In this case I do this to compute Macro-F1, which averages per-class F1 scores\n",
    "    \"\"\"\n",
    "\n",
    "    f1_metric_info: EvaluationModuleInfo = evaluate.load(\"f1\")._info()\n",
    "\n",
    "    def _info(self) -> EvaluationModuleInfo:\n",
    "        # we'll just say the info is the same in this case\n",
    "        return MyMacroF1Metric.f1_metric_info\n",
    "\n",
    "    def _compute(\n",
    "        self,\n",
    "        predictions=None,\n",
    "        references=None,\n",
    "        labels=None,\n",
    "        pos_label=1,\n",
    "        sample_weight=None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        # we can just call the sklearn implementation! Metrics in huggingface generally correspond with sklearn metrics\n",
    "        # when applicable\n",
    "        score = f1_score(\n",
    "            references,\n",
    "            predictions,\n",
    "            labels=labels,\n",
    "            pos_label=pos_label,\n",
    "            average=\"macro\",\n",
    "            sample_weight=sample_weight,\n",
    "        )\n",
    "        return {\"f1\": float(score) if score.size == 1 else score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ea59284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"import evaluate\\n\\n# my_evaluation = evaluate.combine([\\\"accuracy\\\", \\\"f1\\\"])\\n\\nmy_evaluation = evaluate.combine(\\n    [\\n        evaluate.load(\\\"accuracy\\\", average=\\\"weighted\\\"),\\n        MyMacroF1Metric(),\\n    ]\\n)\";\n",
       "                var nbb_formatted_code = \"import evaluate\\n\\n# my_evaluation = evaluate.combine([\\\"accuracy\\\", \\\"f1\\\"])\\n\\nmy_evaluation = evaluate.combine(\\n    [\\n        evaluate.load(\\\"accuracy\\\", average=\\\"weighted\\\"),\\n        MyMacroF1Metric(),\\n    ]\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "# my_evaluation = evaluate.combine([\"accuracy\", \"f1\"])\n",
    "\n",
    "my_evaluation = evaluate.combine(\n",
    "    [\n",
    "        evaluate.load(\"accuracy\", average=\"weighted\"),\n",
    "        MyMacroF1Metric(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19e3af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"from transformers import TrainingArguments\\nfrom sklearn.metrics import accuracy_score\\n\\ntraining_args: TrainingArguments = TrainingArguments(\\n    output_dir=\\\"/data/users/sgarg6/hf_models/quest-4\\\",\\n    do_train=True,\\n    do_eval=True,\\n    do_predict=True,\\n    evaluation_strategy=\\\"steps\\\",\\n    eval_steps=256,\\n    per_device_train_batch_size=256,\\n    per_device_eval_batch_size=256,\\n    save_steps=256,\\n    save_strategy=\\\"steps\\\",\\n    save_total_limit=5,\\n    report_to=[\\\"wandb\\\"],\\n    logging_steps=50,\\n    num_train_epochs=1,\\n    metric_for_best_model=\\\"accuracy\\\",\\n    load_best_model_at_end=True,\\n    dataloader_num_workers=0,  # set to 0 when debugging and >1 when running!\\n)\";\n",
       "                var nbb_formatted_code = \"from transformers import TrainingArguments\\nfrom sklearn.metrics import accuracy_score\\n\\ntraining_args: TrainingArguments = TrainingArguments(\\n    output_dir=\\\"/data/users/sgarg6/hf_models/quest-4\\\",\\n    do_train=True,\\n    do_eval=True,\\n    do_predict=True,\\n    evaluation_strategy=\\\"steps\\\",\\n    eval_steps=256,\\n    per_device_train_batch_size=256,\\n    per_device_eval_batch_size=256,\\n    save_steps=256,\\n    save_strategy=\\\"steps\\\",\\n    save_total_limit=5,\\n    report_to=[\\\"wandb\\\"],\\n    logging_steps=50,\\n    num_train_epochs=1,\\n    metric_for_best_model=\\\"accuracy\\\",\\n    load_best_model_at_end=True,\\n    dataloader_num_workers=0,  # set to 0 when debugging and >1 when running!\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "training_args: TrainingArguments = TrainingArguments(\n",
    "    output_dir=\"/data/users/sgarg6/hf_models/quest-4\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=256,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    save_steps=256,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=5,\n",
    "    report_to=[\"wandb\"],\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    "    dataloader_num_workers=0,  # set to 0 when debugging and >1 when running!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16c929ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"from transformers import EvalPrediction\\nfrom typing import Dict, Tuple\\n\\n\\ndef my_compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\\n    logits, labels = eval_pred.predictions, eval_pred.label_ids\\n    predictions: Tensor = logits.argmax(axis=1)\\n    return my_evaluation.compute(predictions=predictions, references=labels)\";\n",
       "                var nbb_formatted_code = \"from transformers import EvalPrediction\\nfrom typing import Dict, Tuple\\n\\n\\ndef my_compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\\n    logits, labels = eval_pred.predictions, eval_pred.label_ids\\n    predictions: Tensor = logits.argmax(axis=1)\\n    return my_evaluation.compute(predictions=predictions, references=labels)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import EvalPrediction\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "\n",
    "def my_compute_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    logits, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "    predictions: Tensor = logits.argmax(axis=1)\n",
    "    return my_evaluation.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ddcc574f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"from transformers import Trainer\\n\\ntrainer: Trainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=None,  # let HF set this to an instance of transformers.DataCollatorWithPadding\\n    train_dataset=dataset[\\\"train\\\"],\\n    eval_dataset=dataset[\\\"validation\\\"],\\n    tokenizer=tokenizer,\\n    compute_metrics=my_compute_metrics,\\n)\";\n",
       "                var nbb_formatted_code = \"from transformers import Trainer\\n\\ntrainer: Trainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    data_collator=None,  # let HF set this to an instance of transformers.DataCollatorWithPadding\\n    train_dataset=dataset[\\\"train\\\"],\\n    eval_dataset=dataset[\\\"validation\\\"],\\n    tokenizer=tokenizer,\\n    compute_metrics=my_compute_metrics,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer: Trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=None,  # let HF set this to an instance of transformers.DataCollatorWithPadding\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=my_compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "032a3978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/soe/sgarg6/conda/envs/nlp_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 549367\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 256\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2146\n",
      "  Number of trainable parameters = 68097027\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2146' max='2146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2146/2146 09:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.571900</td>\n",
       "      <td>0.673840</td>\n",
       "      <td>0.717334</td>\n",
       "      <td>0.716320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.637535</td>\n",
       "      <td>0.731559</td>\n",
       "      <td>0.731528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.651300</td>\n",
       "      <td>0.616976</td>\n",
       "      <td>0.745682</td>\n",
       "      <td>0.745820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>0.642000</td>\n",
       "      <td>0.610716</td>\n",
       "      <td>0.747815</td>\n",
       "      <td>0.748559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.631300</td>\n",
       "      <td>0.587332</td>\n",
       "      <td>0.758281</td>\n",
       "      <td>0.758691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>0.619600</td>\n",
       "      <td>0.577981</td>\n",
       "      <td>0.760516</td>\n",
       "      <td>0.760139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.569904</td>\n",
       "      <td>0.764377</td>\n",
       "      <td>0.764184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2048</td>\n",
       "      <td>0.608800</td>\n",
       "      <td>0.569622</td>\n",
       "      <td>0.763666</td>\n",
       "      <td>0.763428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-256\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-256/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-256/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-256/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-256/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-512\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-512/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-512/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-512/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-512/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-768\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-768/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-768/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-768/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-768/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-1024\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1024/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1024/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1024/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1024/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-1280\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1280/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1280/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1280/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1280/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-1536\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1536/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1536/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1536/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1536/special_tokens_map.json\n",
      "Deleting older checkpoint [/data/users/sgarg6/hf_models/quest-4/checkpoint-256] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-1792\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1792/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1792/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1792/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-1792/special_tokens_map.json\n",
      "Deleting older checkpoint [/data/users/sgarg6/hf_models/quest-4/checkpoint-512] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9842\n",
      "  Batch size = 256\n",
      "Saving model checkpoint to /data/users/sgarg6/hf_models/quest-4/checkpoint-2048\n",
      "Configuration saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-2048/config.json\n",
      "Model weights saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-2048/pytorch_model.bin\n",
      "tokenizer config file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-2048/tokenizer_config.json\n",
      "Special tokens file saved in /data/users/sgarg6/hf_models/quest-4/checkpoint-2048/special_tokens_map.json\n",
      "Deleting older checkpoint [/data/users/sgarg6/hf_models/quest-4/checkpoint-768] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /data/users/sgarg6/hf_models/quest-4/checkpoint-1792 (score: 0.7643771591140012).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2146, training_loss=0.6380337664487755, metrics={'train_runtime': 585.4445, 'train_samples_per_second': 938.376, 'train_steps_per_second': 3.666, 'total_flos': 6145437024914634.0, 'train_loss': 0.6380337664487755, 'epoch': 1.0})"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 111;\n",
       "                var nbb_unformatted_code = \"trainer.train()\";\n",
       "                var nbb_formatted_code = \"trainer.train()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8575ce86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 112;\n",
       "                var nbb_unformatted_code = \"model = trainer.model\";\n",
       "                var nbb_formatted_code = \"model = trainer.model\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3f18ea58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `CamembertForSequenceClassification.forward` and have been ignored: translated_premise, text, premise, hypothesis, translated_hypothesis. If translated_premise, text, premise, hypothesis, translated_hypothesis are not expected by `CamembertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9824\n",
      "  Batch size = 256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.5801504850387573,\n",
       " 'test_accuracy': 0.7595684039087948,\n",
       " 'test_f1': 0.7595466066589128,\n",
       " 'test_runtime': 4.3199,\n",
       " 'test_samples_per_second': 2274.126,\n",
       " 'test_steps_per_second': 9.028,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 113;\n",
       "                var nbb_unformatted_code = \"trainer.evaluate(metric_key_prefix=\\\"test\\\", eval_dataset=dataset[\\\"test\\\"])\";\n",
       "                var nbb_formatted_code = \"trainer.evaluate(metric_key_prefix=\\\"test\\\", eval_dataset=dataset[\\\"test\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(metric_key_prefix=\"test\", eval_dataset=dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6fa4bcab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847465d84f1042489d922f00eca1abad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▄▄▄▄▁▄▅▅▄▅▆▇▇████</td></tr><tr><td>eval/f1</td><td>▂▂▁▂▄▆▆▇███</td></tr><tr><td>eval/loss</td><td>▄▄▅▅▅▅█▅▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▁▂▂▂▃▂▂█▇▂▃▄▂▂▂▂▂▃</td></tr><tr><td>eval/samples_per_second</td><td>██▇▇▇▅▇▇▁▂▆▆▅▇▇▆▇▆▆</td></tr><tr><td>eval/steps_per_second</td><td>██▇▇▇▅▇▇▁▂▆▆▅▇▇▆▇▆▆</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>test/runtime</td><td>▁</td></tr><tr><td>test/samples_per_second</td><td>▁</td></tr><tr><td>test/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▁▁▁▁▁▂▁▂▁▁▁▂▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▁▁▁▁▁▂▁▂▁▁▁▂▁▁▂▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>██▇▇██▇██▇██▇██▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>▂▂▁▁▆▅▅▅▃▃█▅▅▅▄▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▃▄▃▃▃▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.76367</td></tr><tr><td>eval/f1</td><td>0.76343</td></tr><tr><td>eval/loss</td><td>0.56962</td></tr><tr><td>eval/runtime</td><td>4.111</td></tr><tr><td>eval/samples_per_second</td><td>2394.071</td></tr><tr><td>eval/steps_per_second</td><td>9.487</td></tr><tr><td>test/accuracy</td><td>0.75957</td></tr><tr><td>test/f1</td><td>0.75955</td></tr><tr><td>test/loss</td><td>0.58015</td></tr><tr><td>test/runtime</td><td>4.3199</td></tr><tr><td>test/samples_per_second</td><td>2274.126</td></tr><tr><td>test/steps_per_second</td><td>9.028</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2146</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6162</td></tr><tr><td>train/total_flos</td><td>6145437024914634.0</td></tr><tr><td>train/train_loss</td><td>0.63803</td></tr><tr><td>train/train_runtime</td><td>585.4445</td></tr><tr><td>train/train_samples_per_second</td><td>938.376</td></tr><tr><td>train/train_steps_per_second</td><td>3.666</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">solar-wind-5</strong> at: <a href='https://wandb.ai/sugam110795/nlp244/runs/5ww107vn' target=\"_blank\">https://wandb.ai/sugam110795/nlp244/runs/5ww107vn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230219_113853-5ww107vn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 114;\n",
       "                var nbb_unformatted_code = \"wandb.finish()\";\n",
       "                var nbb_formatted_code = \"wandb.finish()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
